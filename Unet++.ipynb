{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1c6OlKsrYrn27Tc35ImtxZ2AqbTDWZmsy",
      "authorship_tag": "ABX9TyOx1+UDNozLHL6mgSM23kYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchyuthNamburi/webDev/blob/main/Unet%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xs3LpTgN9fy",
        "outputId": "679d8e69-7325-49eb-8d87-1858fb4b9764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Datasets extracted.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "kvasir_zip = '/content/drive/MyDrive/datasets/archive.zip'   # Kvasir\n",
        "polydb_zip = '/content/drive/MyDrive/datasets/PolypDB.zip'   # PolyDB\n",
        "\n",
        "!unzip -q \"$kvasir_zip\" -d ./kvasir/\n",
        "!unzip -q \"$polydb_zip\" -d ./polydb/\n",
        "\n",
        "print(\"Datasets extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python tqdm --quiet\n"
      ],
      "metadata": {
        "id": "iWkDqn1qOg18"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage import color\n",
        "\n",
        "def clahe_lab(img):\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l2 = clahe.apply(l)\n",
        "    lab = cv2.merge((l2,a,b))\n",
        "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "def preprocess_dataset(img_dir, mask_dir, save_img_dir, save_mask_dir, size=(256,256)):\n",
        "    os.makedirs(save_img_dir, exist_ok=True)\n",
        "    os.makedirs(save_mask_dir, exist_ok=True)\n",
        "\n",
        "    images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg','.png'))]\n",
        "\n",
        "    for fname in tqdm(images, desc=f\"Processing {img_dir}\"):\n",
        "        img_path  = os.path.join(img_dir, fname)\n",
        "        mask_path = os.path.join(mask_dir, fname)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        mask = cv2.imread(mask_path, 0)\n",
        "\n",
        "        if img is None or mask is None:\n",
        "            continue\n",
        "\n",
        "        # ---- Image preprocessing\n",
        "        img = cv2.resize(img, size)\n",
        "        img = clahe_lab(img)\n",
        "        img = cv2.GaussianBlur(img, (3,3), 0)\n",
        "\n",
        "        # ---- Mask preprocessing\n",
        "        mask = cv2.resize(mask, size)\n",
        "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        cv2.imwrite(os.path.join(save_img_dir, fname), img)\n",
        "        cv2.imwrite(os.path.join(save_mask_dir, fname), mask)\n"
      ],
      "metadata": {
        "id": "Ibczx0oHPcsa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "# paths\n",
        "\n",
        "\n",
        "kvasir_img = \"/content/kvasir/Kvasir-SEG/Kvasir-SEG/images/\"\n",
        "kvasir_mask = \"/content/kvasir/Kvasir-SEG/Kvasir-SEG/masks/\"\n",
        "\n",
        "polydb_root = \"/content/polydb/PolypDB/PolypDB_modality_wise\"\n",
        "\n",
        "combined_img = \"/content/drive/MyDrive/datasets/combined_dataset/images\"\n",
        "combined_mask = \"/content/drive/MyDrive/datasets/combined_dataset/masks\"\n",
        "\n",
        "os.makedirs(combined_img, exist_ok=True)\n",
        "os.makedirs(combined_mask, exist_ok=True)\n",
        "\n",
        "# --- Copy Kvasir ---\n",
        "for img_path in glob(os.path.join(kvasir_img, \"*.jpg\")):\n",
        "    fname = \"kvasir_\" + os.path.basename(img_path)\n",
        "    shutil.copy(img_path, os.path.join(combined_img, fname))\n",
        "\n",
        "for mask_path in glob(os.path.join(kvasir_mask, \"*.jpg\")):\n",
        "    fname = \"kvasir_\" + os.path.basename(mask_path)\n",
        "    shutil.copy(mask_path, os.path.join(combined_mask, fname))\n",
        "\n",
        "# --- Copy PolypDB (all modalities) ---\n",
        "modalities = [\"BLI\", \"FICE\", \"LCI\", \"NBI\", \"WLI\"]\n",
        "for mod in modalities:\n",
        "    img_dir = os.path.join(polydb_root, mod, \"images\")\n",
        "    mask_dir = os.path.join(polydb_root, mod, \"masks\")\n",
        "\n",
        "    for img_path in glob(os.path.join(img_dir, \"*.jpg\")):\n",
        "        fname = f\"polydb_{mod}_\" + os.path.basename(img_path)\n",
        "        shutil.copy(img_path, os.path.join(combined_img, fname))\n",
        "\n",
        "    for mask_path in glob(os.path.join(mask_dir, \"*.jpg\")):\n",
        "        fname = f\"polydb_{mod}_\" + os.path.basename(mask_path)\n",
        "        shutil.copy(mask_path, os.path.join(combined_mask, fname))\n",
        "\n",
        "print(\"✅ Combined dataset created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i93vjOznVh4T",
        "outputId": "792ada66-3c8b-444f-9ee9-7b065a03c116"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined dataset created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Kvasir dataset\n",
        "kvasir_img_dir = \"/content/kvasir/Kvasir-SEG/Kvasir-SEG/images/\"\n",
        "kvasir_mask_dir = \"/content/kvasir/Kvasir-SEG/Kvasir-SEG/masks/\"\n",
        "\n",
        "\n",
        "# ✅ PolypDB dataset (modality-wise)\n",
        "polydb_modality_dir = \"/content/polydb/PolypDB/PolypDB_modality_wise/\"\n",
        "\n",
        "# ✅ Output combined dataset (you can change the name if you want)\n",
        "merged_img_dir = \"/content/combined_split/images/\"\n",
        "merged_mask_dir = \"/content/combined_split/masks/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Qg_NZF_zY-jU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure these variables are set before running\n",
        "# kvasir_img_dir = your actual path\n",
        "# kvasir_mask_dir = your actual path\n",
        "# polydb_modality_dir = your actual path\n",
        "# merged_img_dir = your actual path\n",
        "# merged_mask_dir = your actual path\n",
        "\n",
        "os.makedirs(merged_img_dir, exist_ok=True)\n",
        "os.makedirs(merged_mask_dir, exist_ok=True)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "# Merge Kvasir\n",
        "for fname in sorted(os.listdir(kvasir_img_dir)):\n",
        "    img_path = os.path.join(kvasir_img_dir, fname)\n",
        "    mask_path = os.path.join(kvasir_mask_dir, fname)\n",
        "    if os.path.exists(mask_path):\n",
        "        new_name = f'kvasir_{counter:05d}.png'\n",
        "        shutil.copy(img_path, os.path.join(merged_img_dir, new_name))\n",
        "        shutil.copy(mask_path, os.path.join(merged_mask_dir, new_name))\n",
        "        counter += 1\n",
        "\n",
        "# Merge PolyDB modality-wise folders\n",
        "for modality in os.listdir(polydb_modality_dir):\n",
        "    modality_img_dir = os.path.join(polydb_modality_dir, modality, \"images\")\n",
        "    modality_mask_dir = os.path.join(polydb_modality_dir, modality, \"masks\")\n",
        "    if not os.path.exists(modality_img_dir) or not os.path.exists(modality_mask_dir):\n",
        "        continue\n",
        "    for fname in sorted(os.listdir(modality_img_dir)):\n",
        "        img_path = os.path.join(modality_img_dir, fname)\n",
        "        mask_path = os.path.join(modality_mask_dir, fname)\n",
        "        if os.path.exists(mask_path):\n",
        "            new_name = f'polydb_{modality}_{counter:05d}.png'\n",
        "            shutil.copy(img_path, os.path.join(merged_img_dir, new_name))\n",
        "            shutil.copy(mask_path, os.path.join(merged_mask_dir, new_name))\n",
        "            counter += 1\n",
        "\n",
        "print(\"Merge complete!\")\n",
        "print(f\"Total images merged: {len(os.listdir(merged_img_dir))}\")\n",
        "print(f\"Total masks merged: {len(os.listdir(merged_mask_dir))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXdGrcX2a5zA",
        "outputId": "558ed2c6-466e-4512-dd17-1e07c369664f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge complete!\n",
            "Total images merged: 1088\n",
            "Total masks merged: 1088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kvasir\n",
        "print(\"Kvasir images:\", len(os.listdir(kvasir_img_dir)))\n",
        "print(\"Kvasir masks:\", len(os.listdir(kvasir_mask_dir)))\n",
        "\n",
        "# PolyDB, count recursively per modality\n",
        "import glob\n",
        "for modality in os.listdir(polydb_modality_dir):\n",
        "    img_path = os.path.join(polydb_modality_dir, modality, \"images\")\n",
        "    mask_path = os.path.join(polydb_modality_dir, modality, \"masks\")\n",
        "    n_imgs = len(glob.glob(os.path.join(img_path, '*')))\n",
        "    n_masks = len(glob.glob(os.path.join(mask_path, '*')))\n",
        "    print(f\"{modality}: images={n_imgs}, masks={n_masks}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXGqWsbUa8O9",
        "outputId": "22f44c5b-c7ac-44ed-9c85-d73a23247d6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kvasir images: 1000\n",
            "Kvasir masks: 1000\n",
            "FICE: images=70, masks=70\n",
            "LCI: images=60, masks=60\n",
            "NBI: images=146, masks=146\n",
            "BLI: images=70, masks=70\n",
            "WLI: images=3588, masks=3588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdyjDQKe7-9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8cteRAW78nz",
        "outputId": "65fa89b8-99e5-4c2c-ef31-17f4278f3219"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWFOxSor7_6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "merged_img_dir = \"/content/drive/MyDrive/datasets/combined_dataset/images/\"\n",
        "merged_mask_dir = \"/content/drive/MyDrive/datasets/combined_dataset/masks/\"\n",
        "\n",
        "split_base = \"/content/combined_split2/\"\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(os.path.join(split_base, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(split_base, split, \"masks\"), exist_ok=True)\n",
        "\n",
        "# 1. Gather and shuffle all files\n",
        "all_files = sorted(os.listdir(merged_img_dir))\n",
        "random.shuffle(all_files)\n",
        "\n",
        "# 2. Train/Val/Test Split (adjust ratios as needed)\n",
        "trainval, test = train_test_split(all_files, test_size=0.10, random_state=42)  # 10% for test\n",
        "train, val = train_test_split(trainval, test_size=0.1667, random_state=42)  # 16.67% of trainval ≈ 15%\n",
        "\n",
        "# 3. Helper to copy files\n",
        "def copy_split(files, split_name):\n",
        "    for fname in files:\n",
        "        shutil.copy(os.path.join(merged_img_dir, fname), os.path.join(split_base, split_name, \"images\", fname))\n",
        "        shutil.copy(os.path.join(merged_mask_dir, fname), os.path.join(split_base, split_name, \"masks\", fname))\n",
        "\n",
        "# 4. Move\n",
        "copy_split(train, \"train\")\n",
        "copy_split(val, \"val\")\n",
        "copy_split(test, \"test\")\n",
        "\n",
        "print(f\"Data split complete. Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n"
      ],
      "metadata": {
        "id": "d3SkrcgbgeEq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d511986b-298a-4e02-81e0-e7d4649e2040"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/combined_dataset/masks/polydb_NBI_d74c8c13-d28b-4e46-964f-54a8b78135c5.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2023214142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 4. Move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcopy_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mcopy_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcopy_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2023214142.py\u001b[0m in \u001b[0;36mcopy_split\u001b[0;34m(files, split_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_img_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_mask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"masks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 4. Move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/combined_dataset/masks/polydb_NBI_d74c8c13-d28b-4e46-964f-54a8b78135c5.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "merged_img_dir = \"/content/drive/MyDrive/datasets/combined_dataset/images/\"\n",
        "merged_mask_dir = \"/content/drive/MyDrive/datasets/combined_dataset/masks/\"\n",
        "\n",
        "split_base = \"/content/combined_split2/\"\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(os.path.join(split_base, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(split_base, split, \"masks\"), exist_ok=True)\n",
        "\n",
        "# Ensure matching pairs only (check extensions)\n",
        "all_imgs = sorted(os.listdir(merged_img_dir))\n",
        "all_masks = sorted(os.listdir(merged_mask_dir))\n",
        "pairs = []\n",
        "for img in all_imgs:\n",
        "    mask = img  # since you copied with same name for both\n",
        "    if mask in all_masks:\n",
        "        pairs.append(img)\n",
        "\n",
        "print(f\"Total pairs for splitting: {len(pairs)}\")\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(pairs)\n",
        "trainval, test = train_test_split(pairs, test_size=0.10, random_state=42)\n",
        "train, val = train_test_split(trainval, test_size=0.1667, random_state=42)\n",
        "\n",
        "def copy_split(file_list, split_name):\n",
        "    for fname in file_list:\n",
        "        shutil.copy(os.path.join(merged_img_dir, fname), os.path.join(split_base, split_name, \"images\", fname))\n",
        "        shutil.copy(os.path.join(merged_mask_dir, fname), os.path.join(split_base, split_name, \"masks\", fname))\n",
        "\n",
        "copy_split(train, \"train\")\n",
        "copy_split(val, \"val\")\n",
        "copy_split(test, \"test\")\n",
        "\n",
        "print(f\"Data split done. Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRPzNNRX5_hG",
        "outputId": "044eeab8-b86c-4704-f4d2-edfa3110019d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs for splitting: 1086\n",
            "Data split done. Train: 814, Val: 163, Test: 109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "merged_img_dir = \"/content/drive/MyDrive/datasets/combined_dataset/images/\"\n",
        "merged_mask_dir = \"/content/drive/MyDrive/datasets/combined_dataset/masks/\"\n",
        "\n",
        "split_base = \"/content/combined_split2/\"\n",
        "\n",
        "# Ensure splits exist\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    os.makedirs(os.path.join(split_base, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(split_base, split, \"masks\"), exist_ok=True)\n",
        "\n",
        "# Build pairs by matching filenames\n",
        "image_files = sorted([f for f in os.listdir(merged_img_dir) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "mask_files  = sorted([f for f in os.listdir(merged_mask_dir) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "\n",
        "pairs = [f for f in image_files if f in mask_files]\n",
        "print(\"Total valid pairs available:\", len(pairs))\n",
        "\n",
        "# Shuffle and split (adjust ratio if needed)\n",
        "random.seed(42)\n",
        "random.shuffle(pairs)\n",
        "\n",
        "trainval, test = train_test_split(pairs, test_size=0.10, random_state=42)\n",
        "train, val = train_test_split(trainval, test_size=0.1667, random_state=42)  # ~15% val\n",
        "\n",
        "def copy_pair_list(file_list, src_img_dir, src_mask_dir, dst_base, split_name):\n",
        "    for fname in file_list:\n",
        "        shutil.copy(os.path.join(src_img_dir, fname),\n",
        "                    os.path.join(dst_base, split_name, \"images\", fname))\n",
        "        shutil.copy(os.path.join(src_mask_dir, fname),\n",
        "                    os.path.join(dst_base, split_name, \"masks\", fname))\n",
        "\n",
        "copy_pair_list(train, merged_img_dir, merged_mask_dir, split_base, \"train\")\n",
        "copy_pair_list(val, merged_img_dir, merged_mask_dir, split_base, \"val\")\n",
        "copy_pair_list(test, merged_img_dir, merged_mask_dir, split_base, \"test\")\n",
        "\n",
        "print(\"Splitting done. Train:\", len(train), \"Val:\", len(val), \"Test:\", len(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fi37m7I86sb",
        "outputId": "baf0891d-8dc5-47fb-c945-7cd4e3304122"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid pairs available: 1086\n",
            "Splitting done. Train: 814 Val: 163 Test: 109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libs\n",
        "!pip install albumentations --quiet\n",
        "\n",
        "import cv2, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class PolypSegDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, augment=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(os.path.join(self.img_dir, self.files[idx]))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(os.path.join(self.mask_dir, self.files[idx]), 0)\n",
        "        mask = (mask > 127).astype(np.float32)\n",
        "\n",
        "        if self.augment:\n",
        "            aug = self.augment(image=img, mask=mask)\n",
        "            img, mask = aug['image'], aug['mask']\n",
        "\n",
        "        img = ToTensorV2()(image=img)['image']\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
        "        return img, mask\n",
        "\n",
        "# Augmentations\n",
        "train_aug = A.Compose([\n",
        "    A.Resize(256,256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.CLAHE(p=1.0),\n",
        "    ToTensorV2()\n",
        "])\n",
        "val_aug = A.Compose([A.Resize(256,256), ToTensorV2()])\n",
        "\n",
        "# Dataloaders\n",
        "train_ds = PolypSegDataset(\"/content/combined_split2/train/images\",\"/content/combined_split2/train/masks\", augment=train_aug)\n",
        "val_ds   = PolypSegDataset(\"/content/combined_split2/val/images\",\" /content/combined_split2/val/masks\", augment=val_aug)\n",
        "test_ds  = PolypSegDataset(\"/content/combined_split2/test/images\",\"/content/combined_split2/test/masks\", augment=val_aug)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "stGgqF8H9Yf4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch --quiet\n",
        "\n",
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# Use a lightweight backbone\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1,\n",
        "    activation=None,\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = smp.losses.DiceLoss(mode=\"binary\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "RB4wK6DO9hkR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "def validate():\n",
        "    model.eval()\n",
        "    tot_loss, tot_batches = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            preds = model(imgs)\n",
        "            loss = loss_fn(preds, masks)\n",
        "            tot_loss += loss.item()\n",
        "            tot_batches += 1\n",
        "    return tot_loss / tot_batches\n",
        "\n",
        "for epoch in range(1, 11):  # 10 epochs as a starting point\n",
        "    tl = train_one_epoch(epoch)\n",
        "    vl = validate()\n",
        "    print(f\"Epoch {epoch}: TrainDice/TrainLoss approx {tl:.4f}, ValLoss={vl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "aVX4iWs99pp8",
        "outputId": "cc6ad65f-cda6-472a-c89c-3e21f7ee1ab0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-403634840.py\", line 30, in __getitem__\n    mask = torch.from_numpy(mask).unsqueeze(0)\n           ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected np.ndarray (got Tensor)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2854552431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 10 epochs as a starting point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: TrainDice/TrainLoss approx {tl:.4f}, ValLoss={vl:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2854552431.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipython-input-403634840.py\", line 30, in __getitem__\n    mask = torch.from_numpy(mask).unsqueeze(0)\n           ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected np.ndarray (got Tensor)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cbdf4e0",
        "outputId": "c505e089-28b0-47e9-b136-7545ce072b14"
      },
      "source": [
        "# Re-run the data loading cell with the corrected augmentations\n",
        "# This cell was modified to fix the ToTensorV2 placement\n",
        "# cell_id: stGgqF8H9Yf4\n",
        "\n",
        "# Dataloaders\n",
        "train_ds = PolypSegDataset(\"/content/combined_split2/train/images\",\"/content/combined_split2/train/masks\", augment=train_aug)\n",
        "val_ds   = PolypSegDataset(\"/content/combined_split2/val/images\",\" /content/combined_split2/val/masks\", augment=val_aug)\n",
        "test_ds  = PolypSegDataset(\"/content/combined_split2/test/images\",\"/content/combined_split2/test/masks\", augment=val_aug)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Dataloaders updated with corrected augmentations.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders updated with corrected augmentations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PolypSegDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, augment=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "        mask_path = os.path.join(self.mask_dir, fname)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Read mask as NumPy array\n",
        "        mask_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask_np is None:\n",
        "            raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
        "        mask_np = (mask_np > 127).astype(np.float32)  # [H,W] -> [0,1]\n",
        "\n",
        "        if self.augment:\n",
        "            augmented = self.augment(image=image, mask=mask_np)\n",
        "            image = augmented['image']\n",
        "            mask_np = augmented['mask']\n",
        "\n",
        "        # Ensure final torch.tensor\n",
        "        # image is already a tensor from ToTensorV2 in your augment, otherwise convert here:\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = torch.from_numpy(image).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        if isinstance(mask_np, np.ndarray):\n",
        "            mask = torch.from_numpy(mask_np).unsqueeze(0)\n",
        "        elif isinstance(mask_np, torch.Tensor):\n",
        "            mask = mask_np.unsqueeze(0)\n",
        "        else:\n",
        "            raise TypeError(\"Mask must be a NumPy array or a Torch tensor at this point\")\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "XRNao0Er93Q4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_aug = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.CLAHE(p=1.0),\n",
        "    # Do not include ToTensorV2 here if you convert in dataset\n",
        "], p=1.0)\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "], p=1.0)\n"
      ],
      "metadata": {
        "id": "uWZYmOQ2-oPE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = PolypSegDataset(\"/content/combined_split2/train/images\",\n",
        "                         \"/content/combined_split2/train/masks\",\n",
        "                         augment=train_aug)\n",
        "\n",
        "val_ds = PolypSegDataset(\"/content/combined_split2/val/images\",\n",
        "                       \"/content/combined_split2/val/masks\",\n",
        "                       augment=val_aug)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qro9nIdI-tPp",
        "outputId": "0cd6b6cc-bcaa-4ecf-fd57-959b4971fc69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1,\n",
        "    activation=None\n",
        ").to(device)\n",
        "\n",
        "loss_fn = smp.losses.DiceLoss(mode=\"binary\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.item())\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def validate():\n",
        "    model.eval()\n",
        "    tot = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            preds = model(imgs)\n",
        "            loss = loss_fn(preds, masks)\n",
        "            tot += float(loss.item())\n",
        "    return tot / len(val_loader)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train_loss = train_one_epoch(epoch)\n",
        "    val_loss = validate()\n",
        "    print(f\"Epoch {epoch} Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Save best model (example)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/models/attention_unetpp.pth\")\n"
      ],
      "metadata": {
        "id": "smCI5EKq-vVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nvpyUZW-zMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}